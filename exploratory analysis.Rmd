---
title: "exploratory analysis"
author: "Andy Cox"
date: "02/12/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text Exploration

Two linraries are needed for this analysus stingi and ggplot2.

```{r}
#character string analysis
library(stringi)
#plotting
library(ggplot2)
#Text analysis 
library(tm)
```

# Abstract
Exploration of three US English text files found online.



# Introduction
In this report we look at three corpora of US English text, a set of internet blogs posts, a set of internet news articles, and a set of twitter messages.
en_US.news.txt
en_blogs.txt
en_US.twitter.txt

We collect the following forms of information:

 1. file size
 2. number of lines
 3. number of words
 4. number of non-empty lines
 5. number of characters
 6. number of non-white characters
 7. distribution of words 

# Data
The data is available as a [ZIP compressed archive](http://en.wikipedia.org/wiki/Zip_(file_format)), which is  downloadable from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r}
setwd()
# specify the source and destination of the download
destination_file <- "Coursera-SwiftKey.zip"
source_file <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# execute the download
download.file(source_file, destination_file)
# extract the files from the zip file
unzip(destination_file)
```

Inspect the unzipped files

```{r}
# find out which files where unzipped
unzip(destination_file, list = TRUE )
```



```{r}
# inspect the data
list.files("final")
list.files("final/en_US")
```


The corpora are contained in three separate plain-text files,
out of which one is binary
Import the files as follows.

```{r}
# import the blogs and twitter datasets in text mode
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding="UTF-8")
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8")
```


```{r}
# import the news dataset in binary mode
con <- file("final/en_US/en_US.news.txt", open="rb")
news <- readLines(con, encoding="UTF-8")
close(con)
rm(con)
```


# Basic Dimsions of the data sources

Determine the size of the files (presented in MegaBytes / MBs).

```{r}
# file size (in MegaBytes/MB)
paste0("File en_US.blogs.txt is ",round(file.info("final/en_US/en_US.blogs.txt")$size   / 1024^2,1), " Megabytes")
```



```{r}
paste0("File en_US.news.txt is ",round(file.info("final/en_US/en_US.news.txt")$size   / 1024^2,1), " Megabytes")
```


```{r}
paste0("File en_US.blogs.txt is ",round(file.info("final/en_US/en_US.twitter.txt")$size   / 1024^2,1), " Megabytes")
```


We analyse the lines and characters.

```{r}
stri_stats_general( blogs )

stri_stats_general( news )

stri_stats_general( twitter )
```



Next we count the words per item (line). We summarise the distibution of these counts per corpus, using summary statistics and a distibution plot. we start with the **blogs** corpus.

```{r}
words_blogs   <- stri_count_words(blogs)
summary( words_blogs )

qplot(   words_blogs,binwidth=10, xlim=c(0,300) )


words_news    <- stri_count_words(news)
summary( words_news )

qplot(   words_news,binwidth=10, xlim=c(0,200)  )

words_twitter <- stri_count_words(twitter)
summary( words_twitter )

qplot(   words_twitter )

#clean up workspace
rm(list=ls())
```

#Data Capstone Exploration, more detailed questions



1. Clean the data from punctuation, stopwords and numbers
2. what are the distributions of word frequencies?
3. What are the frequencies of 2-grams and 3-grams in the dataset?
4. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
5. How do you evaluate how many of the words come from foreign languages?
6. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

#Analyses
1. Clean the data from punctuation, stopwords and numbers

```{r}

# load dataset


#Create some sample files to work wiht more quickly
library(LaF)

blogs1<-sample_lines("final/en_US/en_US.blogs.txt", 100, nlines = NULL)

news1<-sample_lines("final/en_US/en_US.news.txt", 100, nlines = NULL)

twitter1<-sample_lines("final/en_US/en_US.twitter.txt", 100, nlines = NULL)

if(!(dir.exists("final/en_US/sample"))){
    dir.create("final/en_US/sample")

    write(blogs1, file = "final/en_US/sample/en_US.blogs_sample.txt",append = FALSE, sep = "/t")
    write(news1, file = "final/en_US/sample/en_US.news_sample.txt",append = FALSE, sep = "/t")
    write(twitter1, file = "final/en_US/sample/en_US.twitter_sample.txt",append = FALSE, sep = "/t")
    rm(list=c("blogs1","news1","twitter1"))
}

folder.dataset.english <- 'final/en_US/sample'
corpus <- VCorpus(DirSource(directory=folder.dataset.english, encoding = "UTF-8"),  readerControl = list(language = "en"))


# remove URLs
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")

# remove RTs and vias (mostly from tweets)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "RT |via ")

# replace twitter account names by space
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")


# to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# common text cleaning steps
getTransformations()
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# filter profanity

# http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/
terms.to.block <- read.csv(file ='dictionaries/Terms-to-Block.csv', stringsAsFactors=F, skip=3)

terms.to.block <- terms.to.block[,2]
terms.to.block <- unlist(gsub(",", "", terms.to.block))
profanity.filter <- terms.to.block

# filter profanity
corpus <- tm_map(corpus, removeWords, profanity.filter)

# remove double whitespaces
corpus <- tm_map(corpus, stripWhitespace)

# blogs
blogs.dtm <- DocumentTermMatrix(VCorpus(VectorSource(corpus[[1]]$content)))
blogs.dtms <- removeSparseTerms(blogs.dtm , 0.999)
blogs.freq <- sort(colSums(as.matrix(blogs.dtms)), decreasing=TRUE)

# news
news.dtm <- DocumentTermMatrix(VCorpus(VectorSource(corpus[[2]]$content)))
news.dtms <- removeSparseTerms(news.dtm, 0.999)
news.freq <- sort(colSums(as.matrix(news.dtms)), decreasing=TRUE)

# twitter
twitter.dtm <- DocumentTermMatrix(VCorpus(VectorSource(corpus[[3]]$content)))
twitter.dtms <- removeSparseTerms(twitter.dtm , 0.999)
twitter.freq <- sort(colSums(as.matrix(twitter.dtms)), decreasing=TRUE)


```

# Conclusions
We analyse three corpora of US english text. The file sizes are around 200 MegaBytes (MBs) per file.

We find that the **blogs** and **news** corpora consist of about 1 million items each,
and the *twitter** corpus consist of over 2 million items.
Twitter messages have a character limit of 140 (with exceptions for links),
this explains why there are some many more items for a corpus of about the same size.

This result is further supported by the fact that the number of characters is similar for all three corpora (around 200 million each).

Finally we find that the frequency distributions of the **blogs** and **news ** corpora are similar (appearing to be log-normal).
The frequency distribution of the **twitter** corpus is again different, as a result of the character limit.


# References

