---
title: "exploratory analysis"
author: "Andy Cox"
date: "02/12/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text Exploration blog, tweet and news text
Four libraries are needed for this analysus
I will also try out the quantida package fro text analysis
```{r}
#Load Libraries
#character string analysis
library(stringi)
#plotting
library(ggplot2)
#Text analysis 
library(tm)
#Wordcloud
library(wordcloud)
#Fast access to large asci files package
library(LaF)
#Trying out the quanitda package
library(quanteda)
```

# Introduction
We will look at look at the three corpora of US English texts, a set of internet blogs posts, a set of internet news articles, and a set of twitter messages.
en_US.news.txt
en_blogs.txt
en_US.twitter.txt

We collect the following forms of information:

 1. file size
 2. number of lines
 3. number of words
 4. number of non-empty lines
 5. number of characters
 6. number of non-white characters
 7. distribution of words 

# Data
The data is available as a [ZIP compressed archive](http://en.wikipedia.org/wiki/Zip_(file_format)), which is  downloadable from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r}
setwd("/Users/AndyC/Dropbox/rdata/cousera/capstone_git")
# specify the source and destination of the download
destination_file <- "Coursera-SwiftKey.zip"
source_file <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# # execute the download
# download.file(source_file, destination_file)
# # extract the files from the zip file
# unzip(destination_file)
```

Inspect the unzipped files

```{r}
#find out which files where unzipped
unzip(destination_file, list = TRUE )
# inspect the data
list.files("final")
list.files("final/en_US")
```
The corpora are contained in three separate plain-text files,
We caniImport the files as follows.

```{r}
# import the blogs and twitter datasets in text mode
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding="UTF-8")
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8")
news <- readLines("final/en_US/en_US.news.txt", encoding="UTF-8")
```


```{r}
# import the news dataset in binary mode
# con <- file("final/en_US/en_US.news.txt", open="rb")
# news <- readLines(con, encoding="UTF-8")
# close(con)
# rm(con)
```

# Basic Dimsions of the data sources
Determine the size of the files (presented in MegaBytes / MBs).

```{r}
# file size (in MegaBytes/MB)
paste0("File en_US.blogs.txt is ",round(file.info("final/en_US/en_US.blogs.txt")$size   / 1024^2,1), " Megabytes")

paste0("File en_US.news.txt is ",round(file.info("final/en_US/en_US.news.txt")$size   / 1024^2,1), " Megabytes")

paste0("File en_US.blogs.txt is ",round(file.info("final/en_US/en_US.twitter.txt")$size   / 1024^2,1), " Megabytes")
```


We analyse the lines and characters.

```{r}
stri_stats_general( blogs )

stri_stats_general( news )

stri_stats_general( twitter )
```

Next we count the words per item (line). We summarise the distibution of these counts per corpus, using summary statistics and a distibution plot. 

```{r}
words_blogs   <- stri_count_words(blogs)
summary( words_blogs )

qplot(   words_blogs,binwidth=10, xlim=c(0,300) )


words_news    <- stri_count_words(news)
summary( words_news )

qplot(   words_news,binwidth=10, xlim=c(0,200)  )

words_twitter <- stri_count_words(twitter)
summary( words_twitter )

qplot(   words_twitter )

#clean up workspace
rm(list=ls())
```

#Data Capstone Exploration, more detailed questions

1. Clean the data from punctuation, stopwords and numbers
2. what are the distributions of word frequencies?
3. What are the frequencies of 2-grams and 3-grams in the dataset?
4. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
5. How do you evaluate how many of the words come from foreign languages?
6. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

```{r}

# load dataset
#Create some sample files to work wiht more quickly
blogs1<-sample_lines("final/en_US/en_US.blogs.txt", 10000, nlines = NULL)
news1<-sample_lines("final/en_US/en_US.news.txt", 10000, nlines = NULL)
twitter1<-sample_lines("final/en_US/en_US.twitter.txt", 10000, nlines = NULL)

if(!(dir.exists("final/en_US/sample"))){
    dir.create("final/en_US/sample")

    write(blogs1, file = "final/en_US/sample/en_US.blogs_sample.txt",append = FALSE, sep = "/t")
    write(news1, file = "final/en_US/sample/en_US.news_sample.txt",append = FALSE, sep = "/t")
    write(twitter1, file = "final/en_US/sample/en_US.twitter_sample.txt",append = FALSE, sep = "/t")
    rm(list=c("blogs1","news1","twitter1"))
}

folder.dataset.english <- 'final/en_US/sample'
corpus <- VCorpus(DirSource(directory=folder.dataset.english, encoding = "UTF-8"),  readerControl = list(language = "en"))
```
1. Clean the data from punctuation, stopwords and numbers

```{r}
#Analyses

# remove URLs
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")

# remove RTs and vias (mostly from tweets)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "RT |via ")

# replace twitter account names by space
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")


# to lower case
corpus <- tm_map(corpus, content_transformer(tolower))

# common text cleaning steps
getTransformations()
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# filter profanity

# http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/
terms.to.block <- read.csv(file ='dictionaries/Terms-to-Block.csv', stringsAsFactors=F, skip=3)

terms.to.block <- terms.to.block[,2]
terms.to.block <- unlist(gsub(",", "", terms.to.block))
profanity.filter <- terms.to.block

corpus <- tm_map(corpus, removeWords, profanity.filter)

# remove double whitespaces
corpus <- tm_map(corpus, stripWhitespace)
```

3. What are the frequencies of siblge, bigrams and trigrams in the dataset?

```{r}
#clean up
rm(list=ls())

#Thought I would try out the quantida package to see how it goes

folder.dataset.english <- 'final/en_US/sample'

# vector of full filenames for a recursive structure
x<-textfile(list.files(path = folder.dataset.english, pattern = "\\.txt$", 
                    full.names = TRUE, recursive = TRUE))

myCorpus <- corpus(x=x)  # build the corpus
myCorpus<-toLower(myCorpus, keepAcronyms = TRUE)
summary(myCorpus)

myCorpus1<-tokenize(myCorpus, removeNumbers=TRUE, removePunct = TRUE,
  removeSymbols = TRUE, removeSeparators = TRUE, removeTwitter = TRUE,
  removeHyphens = FALSE, removeURL = TRUE, ngrams = 1L, skip = 0L,
  concatenator = "_", simplify = FALSE, verbose = T)


mydfm <- dfm(myCorpus1, ngrams=1,ignoredFeatures = c("the", stopwords("english")))

#Most frequenct words
wds<-dfm(myCorpus, ngrams = 1, verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE,ignoredFeatures=stopwords("english"))
allwds_freq<-sort(colSums(wds),decreasing=T)
wds<-topfeatures(wds, 30)
wds<-data.frame(cbind(names(wds),wds),stringsAsFactors =FALSE)
rownames(wds)<-NULL
colnames(wds)<-c("word","freq")
wds$freq<-as.numeric(wds$freq)

ggplot(wds, aes(reorder(word, freq),freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("Words") + ylab("Frequency") +
  ggtitle("Most frequent Words")


wordcloud(wds$word,wds$freq,max.words=50,rot.per=.3,colors=brewer.pal(8,"Dark2"))

#bigrams
bigrams<-dfm(myCorpus, ngrams = 2,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE,ignoredFeatures=stopwords("english")) 
bigrams<-topfeatures(bigrams, 30)
bigrams<-data.frame(cbind(names(bigrams),bigrams),stringsAsFactors =FALSE)
rownames(bigrams)<-NULL
colnames(bigrams)<-c("word","freq")
bigrams$freq<-as.numeric(bigrams$freq)

ggplot(bigrams, aes(reorder(word,freq), freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("Bigrams") + ylab("Frequency") +
  ggtitle("Most frequent bigrams")


#trigrams
trigrams<-dfm(myCorpus, ngrams = 3,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE,ignoredFeatures=stopwords("english")) 
trigrams<-topfeatures(trigrams, 30) 
trigrams<-data.frame(cbind(names(trigrams),trigrams),stringsAsFactors =FALSE)
rownames(trigrams)<-NULL
colnames(trigrams)<-c("word","freq")
trigrams$freq<-as.numeric(trigrams$freq)

ggplot(trigrams, aes(reorder(word,freq), freq)) +
  geom_bar(stat = "identity") + coord_flip() +
  xlab("Trigrams") + ylab("Frequency") +
  ggtitle("Most frequent trigrams")

```

How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 

```{r}

target1<-ceiling(sum(allwds_freq)*0.5)

for(i in 1:length(allwds_freq)){
sm<-sum(allwds_freq[1:i])
if(sm>=target1)break
}

paste0("Number of unique words needed to cover 50% of all words = ",i)

#Now for 90% of all word instances
target1<-ceiling(sum(allwds_freq)*0.9)

for(i in 1:length(allwds_freq)){
sm<-sum(allwds_freq[1:i])
if(sm>=target1)break
}

paste0("Number of unique words needed to cover 90% of all words = ",i)
```

#How do you evaluate how many of the words come from foreign languages?

You could use and english dictionary to only include words from that language, similarly for other langauges

#Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using #a smaller number of words in the dictionary to cover the same number of phrases?

Can correct common typos, misspellings and abreviations, could also use word stemming to reduce the number of unique words
