---
title: "Modelling Task"
author: "Andy Cox"
date: "24/12/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Details about the approach

The basis of the apporach was inspired by the publication
"Natural Language Processing: A Model to Predict a Sequence of Words"
by Gerald R Gendron et al.  MODSIM Wolrd 2015

Taken from that publication the following approach will be adopted

1. Case: corpora words will not be case-sensitive.
2. Stopwords will not be removed, unlike classification and clustering applications, all words will be included in the
model.

3. Wordform: stemming will not be used

4. Punctuation: Initally punctution will not be used,  but will be consideredas a development of the model. 
Jurafsky and Martin treat punctuation as a word and count it as a word. Given the nature of
the problem, which is not trying to generate full sentences but only predict a next word, punctuation will be
treated slightly differently in the initial model. End of sentence punctuation (e.g., ? ' ! . ) will be used to
include end-of-sentence tags, as the intuition is they have implications for word prediction.

5. Parts of Speech: twill not be used.

6. Numbers: will not be used

7. Sparse Words: all words will be retained. 

8. Whitespace: Will be removed and not used

```{r}
#clean up workspace
rm(list=ls())

#Load Libraries
#character string analysis
library(stringi)
#plotting
library(ggplot2)
#Text analysis 
library(tm)
#Wordcloud
library(wordcloud)
#Fast access to large asci files package
library(LaF)
#Trying out the quanitda package
library(quanteda)


```

##Load the datasets
```{r}
setwd("/Users/AndyC/Dropbox/rdata/cousera/capstone_git")

#Create some sample files to work wiht more quickly
blogs1<-sample_lines("final/en_US/en_US.blogs.txt", 2000, nlines = NULL)
news1<-sample_lines("final/en_US/en_US.news.txt", 2000, nlines = NULL)
twitter1<-sample_lines("final/en_US/en_US.twitter.txt", 2000, nlines = NULL)

if(!(dir.exists("final/en_US/sample"))){
    dir.create("final/en_US/sample")

    write(blogs1, file = "final/en_US/sample/en_US.blogs_sample.txt",append = FALSE, sep = "/t")
    write(news1, file = "final/en_US/sample/en_US.news_sample.txt",append = FALSE, sep = "/t")
    write(twitter1, file = "final/en_US/sample/en_US.twitter_sample.txt",append = FALSE, sep = "/t")
    rm(list=c("blogs1","news1","twitter1"))
}

folder.dataset.english <- 'final/en_US/sample'
corpus <- VCorpus(DirSource(directory=folder.dataset.english, encoding = "UTF-8"),  readerControl = list(language = "en"))

```



```{r}


#Thought I would try out the quantida package to see how it goes

folder.dataset.english <- 'final/en_US/sample'

# vector of full filenames for a recursive structure
x<-textfile(list.files(path = folder.dataset.english, pattern = "\\.txt$", 
                    full.names = TRUE, recursive = TRUE))

myCorpus <- corpus(x=x)  # build the corpus
myCorpus<-toLower(corpus, keepAcronyms = TRUE)
summary(myCorpus)


#Most frequenct words
wds<-dfm(myCorpus, ngrams = 1, verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE)
allwds_freq<-sort(colSums(wds),decreasing=T)

wds<-data.frame(names=names(allwds_freq),freq=allwds_freq,stringsAsFactors =FALSE)
rownames(wds)<-NULL
uni.freq.freq<-table(wds$freq)
saveRDS(wds,"dictionaries/unigrams.rds")
#bigrams
bigrams<-dfm(myCorpus, ngrams = 2,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE) 
bigrams_freq<-sort(colSums(bigrams),decreasing=T)
bigrams<-data.frame(names=names(bigrams_freq),freq=bigrams_freq,stringsAsFactors =FALSE)
bigrams$first<- sapply(strsplit(bigrams$names, "_"), "[[", 1)
bigrams$last<-  sapply(strsplit(bigrams$names, "_"), "[[", 2)
rownames(bigrams)<-NULL
bigrams.freq.freq<-table(bigrams$freq)
saveRDS(bigrams,"dictionaries/bigrams.rds")

#trigrams
trigrams<-dfm(myCorpus, ngrams = 3,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE) 
trigrams_freq<-sort(colSums(trigrams),decreasing=T)
trigrams<-data.frame(names=names(trigrams_freq),freq=trigrams_freq,stringsAsFactors =FALSE)

trigrams$first<-paste(sapply(strsplit(trigrams$names, "_"), "[[", 1),sapply(strsplit(trigrams$names, "_"), "[[", 2),sep="_")
trigrams$last<-sapply(strsplit(trigrams$names, "_"), "[[", 3)
rownames(trigrams)<-NULL
saveRDS(trigrams,"dictionaries/trigrams.rds")
#qgrams
qgrams<-dfm(myCorpus, ngrams = 4,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE) 
qgrams_freq<-sort(colSums(qgrams),decreasing=T)
qgrams<-data.frame(names=names(qgrams_freq),freq=qgrams_freq,stringsAsFactors =FALSE)

qgrams$first<-paste(sapply(strsplit(qgrams$names, "_"), "[[", 1),sapply(strsplit(qgrams$names, "_"), "[[", 2),sapply(strsplit(qgrams$names, "_"), "[[", 3),sep="_")
qgrams$last<-sapply(strsplit(qgrams$names, "_"), "[[", 4)
rownames(qgrams)<-NULL
saveRDS(qgrams,"dictionaries/qgrams.rds")


```

```{r}
d.unigrams<-readRDS("dictionaries/qgrams.rds")

```