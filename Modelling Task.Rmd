---
title: "Modelling Task"
author: "Andy Cox"
date: "24/12/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Details about the approach

The basis of the apporach was inspired by the publication
"Natural Language Processing: A Model to Predict a Sequence of Words"
by Gerald R Gendron et al.  MODSIM Wolrd 2015

Taken from that publication the following approach will be adopted

1. Case: corpora words will not be case-sensitive.
2. Stopwords will not be removed, unlike classification and clustering applications, all words will be included in the
model.

3. Wordform: stemming will not be used

4. Punctuation: Initally punctution will not be used,  but will be consideredas a development of the model. 
Jurafsky and Martin treat punctuation as a word and count it as a word. Given the nature of
the problem, which is not trying to generate full sentences but only predict a next word, punctuation will be
treated slightly differently in the initial model. End of sentence punctuation (e.g., ? ' ! . ) will be used to
include end-of-sentence tags, as the intuition is they have implications for word prediction.

5. Parts of Speech: twill not be used.

6. Numbers: will not be used

7. Sparse Words: all words will be retained. 

8. Whitespace: Will be removed and not used

```{r}
#clean up workspace
rm(list=ls())

#Load Libraries
#character string analysis
library(stringi)
#plotting
library(ggplot2)
#Text analysis 
library(tm)
#Wordcloud
library(wordcloud)
#Fast access to large asci files package
library(LaF)
#Trying out the quanitda package
library(quanteda)
 # package used to update/maintain permanent corpus database
library(filehash)
```

##Load the datasets
```{r}
setwd("/Users/AndyC/Dropbox/rdata/cousera/capstone_git")

#Create some sample files to work wiht more quickly
blogs1<-sample_lines("final/en_US/en_US.blogs.txt", 1000, nlines = NULL)
news1<-sample_lines("final/en_US/en_US.news.txt", 1000, nlines = NULL)
twitter1<-sample_lines("final/en_US/en_US.twitter.txt", 1000, nlines = NULL)

if(!(dir.exists("final/en_US/sample"))){
    dir.create("final/en_US/sample")

    write(blogs1, file = "final/en_US/sample/en_US.blogs_sample.txt",append = FALSE, sep = "/t")
    write(news1, file = "final/en_US/sample/en_US.news_sample.txt",append = FALSE, sep = "/t")
    write(twitter1, file = "final/en_US/sample/en_US.twitter_sample.txt",append = FALSE, sep = "/t")
    rm(list=c("blogs1","news1","twitter1"))
}

folder.dataset.english <- 'final/en_US/sample'
corpus <- VCorpus(DirSource(directory=folder.dataset.english, encoding = "UTF-8"),  readerControl = list(language = "en"))

```



```{r}


#Thought I would try out the quantida package to see how it goes

folder.dataset.english <- 'final/en_US/sample'

# vector of full filenames for a recursive structure
x<-textfile(list.files(path = folder.dataset.english, pattern = "\\.txt$", 
                    full.names = TRUE, recursive = TRUE))

myCorpus <- corpus(x=x)  # build the corpus
myCorpus<-toLower(corpus, keepAcronyms = TRUE)
summary(myCorpus)


#Most frequenct words
wds<-dfm(myCorpus, ngrams = 1, verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE)
allwds_freq<-sort(colSums(wds),decreasing=T)

wds<-data.frame(names=names(allwds_freq),freq=allwds_freq,stringsAsFactors =FALSE)
rownames(wds)<-NULL
uni.freq.freq<-table(wds$freq)

#bigrams
bigrams<-dfm(myCorpus, ngrams = 2,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE) 
bigrams_freq<-sort(colSums(bigrams),decreasing=T)
bigrams<-data.frame(names=names(bigrams_freq),freq=bigrams_freq,stringsAsFactors =FALSE)
rownames(bigrams)<-NULL
bigrams.freq.freq<-table(bigrams$freq)

#trigrams
trigrams<-dfm(myCorpus, ngrams = 3,verbose = TRUE, toLower = TRUE,
  removeNumbers = TRUE, removePunct = TRUE, removeSeparators = TRUE,
  removeTwitter = TRUE, stem = FALSE) 
trigrams_freq<-sort(colSums(trigrams),decreasing=T)
trigrams<-data.frame(names=names(trigrams_freq),freq=trigrams_freq,stringsAsFactors =FALSE)
rownames(trigrams)<-NULL
trigrams.freq.freq<-table(trigrams$freq)
```

```{r}
SimpleGT <- function(table_N){
  #Simple Good Turing Algorithm - Gale And Simpson
  #Good Turing Smoothing

  # table_U is a table of frequency of frequencies
  # The frequencies are stored as names of the list in the table structure
  # the values are the frequency of frequencies.
  # In Good Turing Smoothing, we are concerned with the frequency of frequencies
  # So, to extract the number of times that words of frequency n occur in the training set, we need to do:
  # table(freq_B)[[as.character(pairCount)]]
  # In a tables with a number of holes or non-contiguous sequence of frequency of words,
  # we can compute N(c+1) as the mean of all frequencies that occur more than Nc times
  # to do this, create a vector that is in the numerical form of the names of the table

  # create a data table
  # r is the frequencies of various trigrams
  #n is the frequency of frquencies
  SGT_DT <- data.frame(r=as.numeric(names(table_N)),n=as.vector(table_N),Z=vector("numeric",length(table_N)), 
                           logr=vector("numeric",length(table_N)),
                           logZ=vector("numeric",length(table_N)),
                           r_star=vector("numeric",length(table_N)),
                           p=vector("numeric",length(table_N)))
                          #p=vector("numeric",length(table_N)),key="r")

  str(SGT_DT)
  
  num_r <- nrow(SGT_DT)
  for (j in 1:num_r) {
      if(j==1) {r_i<-0} else {r_i <- SGT_DT$r[j-1]}
      if(j==num_r){r_k<-SGT_DT$r[j]} else {r_k <- SGT_DT$r[j+1]}
      SGT_DT$Z[j] <- 2*SGT_DT$n[j] / (r_k-r_i)
      #print(paste(r_i,j,r_k))
  }
  SGT_DT$logr <- log(SGT_DT$r)
  SGT_DT$logZ <- log(SGT_DT$Z)
  linearFit <- lm(SGT_DT$logZ ~ SGT_DT$logr)
  print(linearFit$coefficients)
  c0 <- linearFit$coefficients[1]
  c1 <- linearFit$coefficients[2]
  
  plot(SGT_DT$logr, SGT_DT$logZ)
  abline(linearFit,col="red")
  
  use_y = FALSE
  for (j in 1:(num_r-1)) {
    r_plus_1 <- SGT_DT$r[j] + 1
    
    s_r_plus_1 <- exp(c0 + (c1 * SGT_DT$logr[j+1]))
    s_r <- exp(c0 + (c1 * SGT_DT$logr[j]))
    y<-r_plus_1 * s_r_plus_1/s_r
    
    if(use_y) {
      SGT_DT$r_star[j] <- y
    } else { 
      n_r_plus_1 <- SGT_DT$n[SGT_DT$r == r_plus_1]
      n_r <- SGT_DT$n[j]
      x<-(r_plus_1) * n_r_plus_1/n_r
      
      if (abs(x-y) > 1.96 * sqrt(((r_plus_1)^2) * (n_r_plus_1/((n_r)^2))*(1+(n_r_plus_1/n_r)))) {
        SGT_DT$r_star[j] <- x
      }else {
        SGT_DT$r_star[j] <- y
        use_y = TRUE
      }
    }
    if(j==(num_r-1)) {
      SGT_DT$r_star[j+1] <- y
    }
        
  }
  N <- sum(SGT_DT$n * SGT_DT$r)
  Nhat <- sum(SGT_DT$n * SGT_DT$r_star)
  Po <- SGT_DT$n[1] / N
  SGT_DT$p <- (1-Po) * SGT_DT$r_star/Nhat
  
  return(SGT_DT)
  
}

# #SGT tables for bigrams and trigrams
tri_SGT_DT <- SimpleGT(bigrams.freq.freq)


```